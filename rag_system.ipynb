{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf424c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (5.1.0)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (0.34.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain) (0.4.17)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.6.4-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-win_amd64.whl.metadata (76 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langchain-huggingface) (0.21.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from sentence-transformers) (4.55.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sanka\\anaconda3\\envs\\learning\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 799.2 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 799.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 645.1 kB/s eta 0:00:00\n",
      "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.8/2.5 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-win_amd64.whl (450 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.6.4-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-win_amd64.whl (86 kB)\n",
      "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.6/18.2 MB 7.0 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/18.2 MB 6.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 3.1/18.2 MB 5.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 4.5/18.2 MB 5.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.0/18.2 MB 5.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.8/18.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 7.9/18.2 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 9.7/18.2 MB 5.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 10.7/18.2 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.5/18.2 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 13.1/18.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.4/18.2 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 15.2/18.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.8/18.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-win_amd64.whl (43 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.3.2-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Installing collected packages: requests, pypdf, propcache, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, faiss-cpu, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-core, langchain-huggingface, langchain, langchain-community\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.32.4\n",
      "\n",
      "    Uninstalling requests-2.32.4:\n",
      "\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "\n",
      "   ----------------------------------------  0/20 [requests]\n",
      "   -- -------------------------------------  1/20 [pypdf]\n",
      "   -- -------------------------------------  1/20 [pypdf]\n",
      "   -- -------------------------------------  1/20 [pypdf]\n",
      "   -- -------------------------------------  1/20 [pypdf]\n",
      "   -- -------------------------------------  1/20 [pypdf]\n",
      "   ---- -----------------------------------  2/20 [propcache]\n",
      "   -------- -------------------------------  4/20 [multidict]\n",
      "   ---------- -----------------------------  5/20 [marshmallow]\n",
      "   ------------ ---------------------------  6/20 [httpx-sse]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   ---------------- -----------------------  8/20 [faiss-cpu]\n",
      "   -------------------- ------------------- 10/20 [yarl]\n",
      "   -------------------------- ------------- 13/20 [pydantic-settings]\n",
      "   -------------------------- ------------- 13/20 [pydantic-settings]\n",
      "   ---------------------------- ----------- 14/20 [dataclasses-json]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "  Attempting uninstall: langchain-core\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "    Found existing installation: langchain-core 0.3.74\n",
      "   ------------------------------ --------- 15/20 [aiohttp]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "    Uninstalling langchain-core-0.3.74:\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "      Successfully uninstalled langchain-core-0.3.74\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   -------------------------------- ------- 16/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-huggingface]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   ------------------------------------ --- 18/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   -------------------------------------- - 19/20 [langchain-community]\n",
      "   ---------------------------------------- 20/20 [langchain-community]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 dataclasses-json-0.6.7 faiss-cpu-1.12.0 frozenlist-1.7.0 httpx-sse-0.4.1 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.75 langchain-huggingface-0.3.1 marshmallow-3.26.1 multidict-6.6.4 mypy-extensions-1.1.0 propcache-0.3.2 pydantic-settings-2.10.1 pypdf-6.0.0 requests-2.32.5 typing-inspect-0.9.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-community langchain-huggingface faiss-cpu sentence-transformers pypdf huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3509110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up HuggingFace API Token\n",
      "==================================================\n",
      "Please enter your HuggingFace API token\n",
      "Get your token from: https://huggingface.co/settings/tokens\n",
      "API token configured successfully!\n"
     ]
    }
   ],
   "source": [
    "#Setup and API Key Configuration\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "def setup_huggingface_api():\n",
    "    print(\"Setting up HuggingFace API Token\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check if token already exists in environment\n",
    "    api_token = os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "    if api_token:\n",
    "        print(\"API token found in environment variables\")\n",
    "        return api_token\n",
    "    else:\n",
    "        print(\"Please enter your HuggingFace API token\")\n",
    "        print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
    "        api_token = getpass(\"Enter your HF API token: \")\n",
    "\n",
    "        os.environ['HUGGINGFACEHUB_API_TOKEN'] = api_token\n",
    "        print(\"API token configured successfully!\")\n",
    "        return api_token\n",
    "\n",
    "api_token = setup_huggingface_api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "946eafd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDF to Vector Database\n",
      "==================================================\n",
      "Loading PDF: Simple Guide to Engineering College Counselling in-1.pdf\n",
      "Loaded 2 pages\n",
      "Splitting document into smaller chunks...\n",
      "✅ Created 4 text chunks\n",
      "Creating embeddings (this converts text to numbers)...\n",
      "Creating vector database...\n",
      "Vector database saved as 'faiss_index'\n",
      "Metadata saved\n",
      "\n",
      "PDF conversion complete!\n",
      "Summary:\n",
      "   -Pages processed: 2\n",
      "   -Chunks created: 4\n",
      "   -Ready for questions!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF to Vectors - Convert PDF into searchable database\n",
    "import os\n",
    "import pickle\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def pdf_to_vectors_simple(pdf_path):\n",
    "    print(\"Converting PDF to Vector Database\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"Loading PDF: {pdf_path}\")\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"File not found: {pdf_path}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        print(f\"Loaded {len(documents)} pages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "    print(\"Splitting document into smaller chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        \n",
    "        chunk_overlap=200,     \n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"✅ Created {len(texts)} text chunks\")\n",
    "\n",
    "\n",
    "    print(\"Creating embeddings (this converts text to numbers)...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "  \n",
    "    print(\"Creating vector database...\")\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    \n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "    print(\"Vector database saved as 'faiss_index'\")\n",
    "\n",
    "    \n",
    "    metadata = {\n",
    "        'total_chunks': len(texts),\n",
    "        'total_pages': len(documents),\n",
    "        'chunk_size': 1000,\n",
    "        'overlap': 200,\n",
    "        'pdf_name': pdf_path\n",
    "    }\n",
    "\n",
    "    with open(\"pdf_metadata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(\"Metadata saved\")\n",
    "\n",
    "    print(\"\\nPDF conversion complete!\")\n",
    "    print(f\"Summary:\")\n",
    "    print(f\"   -Pages processed: {len(documents)}\")\n",
    "    print(f\"   -Chunks created: {len(texts)}\")\n",
    "    print(f\"   -Ready for questions!\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "pdf_to_vectors_simple(\"Simple Guide to Engineering College Counselling in-1.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ed74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question Answering System with memory\n",
    "import os\n",
    "import pickle\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "def initialize_qa_system():\n",
    "    print(\"Initializing Question-Answering System with Memory\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    if not os.path.exists(\"faiss_index\"):\n",
    "        print(\"Vector database not found!\")\n",
    "        print(\"Please run 'pdf_to_vectors_simple()' first\")\n",
    "        return None, None, None\n",
    "\n",
    "    \n",
    "    print(\"Loading vector database...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    vectorstore = FAISS.load_local(\"faiss_index\", embeddings,\n",
    "                                 allow_dangerous_deserialization=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Setting up conversation memory...\")\n",
    "\n",
    "    chat_history = ChatMessageHistory()\n",
    "    memory = ConversationBufferMemory(\n",
    "    chat_memory=chat_history,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(\"Connecting to HuggingFace...\")\n",
    "    try:\n",
    "        api_token = os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "        client = InferenceClient(api_key=api_token)\n",
    "        print(\"HuggingFace client ready!\")\n",
    "    except Exception as e:\n",
    "        print(f\"HuggingFace connection failed: {e}\")\n",
    "        client = None\n",
    "\n",
    "    return vectorstore, client, memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_with_memory(question, vectorstore, client, memory, num_results=5):\n",
    "    print(f\"\\nSearching for: '{question}'\")\n",
    "\n",
    "    \n",
    "    docs_with_scores = vectorstore.similarity_search_with_score(question, k=num_results)\n",
    "\n",
    "    print(f\"Found {len(docs_with_scores)} relevant sections:\")\n",
    "\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, (doc, score) in enumerate(docs_with_scores):\n",
    "        page = doc.metadata.get('page', 'Unknown')\n",
    "        print(f\"   • Section {i+1}: Page {page} (Relevance: {score:.3f})\")\n",
    "        context_parts.append(f\"[Page {page}]: {doc.page_content}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "\n",
    "    \n",
    "    chat_history = memory.chat_memory.messages if memory.chat_memory.messages else []\n",
    "    history_text = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in chat_history[-4:]])\n",
    "\n",
    "   \n",
    "    print(\"Generating answer with conversation context...\")\n",
    "\n",
    "    if client:\n",
    "        try:\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are a helpful assistant. Answer questions based on the provided document context and conversation history. Always mention page numbers when possible.\\n\\nConversation History:\\n{history_text}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Document Context:\\n{context}\\n\\nCurrent Question: {question}\\n\\nPlease provide a clear answer considering both the document and our conversation:\"\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = client.chat_completion(\n",
    "                messages=messages,\n",
    "                model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                max_tokens=300\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            \n",
    "            memory.chat_memory.add_user_message(question)\n",
    "            memory.chat_memory.add_ai_message(answer)\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"AI generation failed: {e}\")\n",
    "            print(\"Providing context-based answer instead...\")\n",
    "\n",
    "    \n",
    "    answer = f\"Based on your question and our conversation, here are the most relevant sections:\\n\\n{context[:1500]}...\"\n",
    "    memory.chat_memory.add_user_message(question)\n",
    "    memory.chat_memory.add_ai_message(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19a77bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Q&A Chat with Memory!\n",
      "==================================================\n",
      "Initializing Question-Answering System with Memory\n",
      "==================================================\n",
      "📂 Loading vector database...\n",
      "Setting up conversation memory...\n",
      "Connecting to HuggingFace...\n",
      "HuggingFace client ready!\n",
      "Loaded: 4 chunks from 2 pages\n",
      "\n",
      "Instructions:\n",
      "   - Ask questions - I'll remember our conversation!\n",
      "   - Try follow-up questions like 'Can you explain that more?'\n",
      "   - Type 'memory' to see conversation history\n",
      "   - Type 'clear' to clear memory\n",
      "   - Type 'quit' to exit\n",
      "==================================================\n",
      "\n",
      "🔍 Searching for: 'what is this pdf about?'\n",
      "📄 Found 4 relevant sections:\n",
      "   • Section 1: Page 0 (Relevance: 1.888)\n",
      "   • Section 2: Page 0 (Relevance: 1.908)\n",
      "   • Section 3: Page 1 (Relevance: 2.010)\n",
      "   • Section 4: Page 1 (Relevance: 2.061)\n",
      "🤖 Generating answer with conversation context...\n",
      "\n",
      "**Answer:**\n",
      "This PDF is about the college counseling process in Tamil Nadu, specifically for engineering college admissions. It provides a simple guide to understanding the process, explaining how marks from Class 12 exams determine college choices and seats are allocated through online counseling rounds.\n",
      "----------------------------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def qa_chat():\n",
    "    print(\"PDF Q&A Chat with Memory!\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    \n",
    "    vectorstore, client, memory = initialize_qa_system()\n",
    "    if vectorstore is None:\n",
    "        return\n",
    "\n",
    "    \n",
    "    try:\n",
    "        with open(\"pdf_metadata.pkl\", \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "        print(f\"Loaded: {metadata['total_chunks']} chunks from {metadata['total_pages']} pages\")\n",
    "    except:\n",
    "        print(\"Metadata not available\")\n",
    "\n",
    "    print(\"\\nInstructions:\")\n",
    "    print(\"   - Ask questions - I'll remember our conversation!\")\n",
    "    print(\"   - Try follow-up questions like 'Can you explain that more?'\")\n",
    "    print(\"   - Type 'memory' to see conversation history\")\n",
    "    print(\"   - Type 'clear' to clear memory\")\n",
    "    print(\"   - Type 'quit' to exit\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\n Your question: \").strip()\n",
    "\n",
    "        if question.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if question.lower() == 'memory':\n",
    "            messages = memory.chat_memory.messages\n",
    "            if messages:\n",
    "                print(\"Conversation History:\")\n",
    "                for i, msg in enumerate(messages[-6:]):  \n",
    "                    role = \"You\" if msg.type == \"human\" else \"Bot\"\n",
    "                    print(f\"   {role}: {msg.content[:100]}...\")\n",
    "            else:\n",
    "                print(\"No conversation history yet\")\n",
    "            continue\n",
    "\n",
    "        if question.lower() == 'clear':\n",
    "            memory.clear()\n",
    "            print(\"Memory cleared!\")\n",
    "            continue\n",
    "\n",
    "        if question.lower() == 'info':\n",
    "            try:\n",
    "                with open(\"pdf_metadata.pkl\", \"rb\") as f:\n",
    "                    metadata = pickle.load(f)\n",
    "                print(f\"System Status:\")\n",
    "                print(f\"   - PDF: {metadata['pdf_name']}\")\n",
    "                print(f\"   - Pages: {metadata['total_pages']}\")\n",
    "                print(f\"   - Chunks: {metadata['total_chunks']}\")\n",
    "                print(f\"   - AI Client: {'Active' if client else 'Using fallback'}\")\n",
    "                print(f\"   - Memory: {len(memory.chat_memory.messages)} messages stored\")\n",
    "            except:\n",
    "                print(\"System info not available\")\n",
    "            continue\n",
    "\n",
    "        if not question:\n",
    "            print(\"Please enter a question!\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        answer = ask_question_with_memory(question, vectorstore, client, memory)\n",
    "\n",
    "        print(f\"\\n**Answer:**\")\n",
    "        print(answer)\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "\n",
    "qa_chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
